{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a72c0e",
   "metadata": {},
   "source": [
    "# The Continuous Thought Machine – Tutorial 03: Mazes [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SakanaAI/continuous-thought-machines/blob/main/examples/01_mnist.ipynb) [![arXiv](https://img.shields.io/badge/arXiv-2505.05522-b31b1b.svg)](https://arxiv.org/abs/2505.05522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cf27b",
   "metadata": {},
   "source": [
    "### Maze Solving\n",
    "\n",
    "In Section 4 of the [technical report](https://arxiv.org/pdf/2505.05522), we showcase how a CTM can be used to solve 2D mazes. Typically in the literature, the task of solving a maze is described as a form of binary classification: by ensuring the output space matches the dimensions of the input space, a model can classify, for each pixel in the input image, if the pixel belongs to the path though the maze. While this approach has seen success, it exludes the need to think in a more natural fashion. We seek to design a more challening task, where a more-human like solution is required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f769e",
   "metadata": {},
   "source": [
    "### Task Details\n",
    "\n",
    "Instead of classifying if each pixel in the maze is or is not along the solution path, we instead constrain the output space, such that the model must output a plan: an entire trajectoru of actions corresponding to the steps an agent must take to go from the start position to goal position. Specifically, for each internal tick of the CTM, the model produces a sequence of actions. These actions correspond to Left, Right, Up, Down and Wait (or no-op). We can then use a cross-entropy loss function which compares these actions produced by the CTM to the ground-truth trajectory required to solve the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2272f18",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257dbd3",
   "metadata": {},
   "source": [
    "In addition to installing some dependencies, we also clone the CTM repo (assuming this tutorial is being ran in Colab), so that we can access the base CTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ccfdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mfatal: destination path 'continuous-thought-machines' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install mediapy\n",
    "!git clone https://github.com/SakanaAI/continuous-thought-machines.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397bf56",
   "metadata": {},
   "source": [
    "Download the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=1Z8FFnZ7pZcu7DfoSyfy-ghWa08lgYl1V\"\n",
    "!unzip \"small-mazes.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab57a96",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24ffe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"./continuous-thought-machines\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# From CTM repo\n",
    "from models.ctm import ContinuousThoughtMachine as CTM\n",
    "from data.custom_datasets import MazeImageFolder\n",
    "from tasks.mazes.plotting import make_maze_gif\n",
    "from tasks.image_classification.plotting import plot_neural_dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407a4a8",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271bc4c",
   "metadata": {},
   "source": [
    "The training (test) folder contains roughly 9000 (1000) images of mazes, at a resolution of 15x15. In these images, the red and green pixels correspond to the start and goal position respectively, while the blue pixels indicate the path through the maze. We will use these blue pixels to construct targets, and remove them from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a7427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_root = './small-mazes'\n",
    "train_dir = f\"{data_root}/train/0\"\n",
    "images = os.listdir(train_dir)[:4]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, img_name in enumerate(images):\n",
    "    img = Image.open(f\"{data_root}/train/0/{img_name}\")\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec4d00",
   "metadata": {},
   "source": [
    "To use these mazes, we have built a custom dataset `MazeImageFolder`. This dataset creates the input-target pairs corresponding to the unsolved mazes images and their solutions. For more details on how this dataset process the images, please refer to it's implementation [here](https://github.com/SakanaAI/continuous-thought-machines/blob/aecfb63ac42db7a20903ee27489ff71671669474/data/custom_datasets.py#L142C7-L142C22).\n",
    "\n",
    "Note that the `MazeImageFolder` takes a `maze_route_length` argument. This parameter specifies the fixed length of action sequences that the CTM will generate at each internal tick. Since different mazes have solutions of varying lengths, all target sequences are padded with the wait action (integer 4) to ensure they match the specified `maze_route_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes:   1%|▏         | 134/9000 [00:00<00:15, 583.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes: 100%|██████████| 9000/9000 [00:15<00:00, 572.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving all mazes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes: 100%|██████████| 1000/1000 [00:01<00:00, 643.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving all mazes...\n"
     ]
    }
   ],
   "source": [
    "train_data = MazeImageFolder(root=f'{data_root}/train/', which_set='train', maze_route_length=50)\n",
    "test_data = MazeImageFolder(root=f'{data_root}/test/', which_set='test', maze_route_length=50)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a149b",
   "metadata": {},
   "source": [
    "We can visualise what these inputs and targets look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33503097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target action sequence:\n",
      "[3, 3, 0, 0, 0, 0, 2, 2, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "sample_inputs, sample_targets = next(iter(testloader))\n",
    "sample_input, sample_target = sample_inputs[0], sample_targets[0]\n",
    "sample_input = sample_input.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(np.clip(sample_input, 0, 1))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Target action sequence:\")\n",
    "print(f\"{sample_target.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d617604",
   "metadata": {},
   "source": [
    "As shown in the above output, the target is a sequence of integers, each corresponding to a unqie action: \n",
    "  - 0: Up\n",
    "  - 1: Down  \n",
    "  - 2: Left\n",
    "  - 3: Right\n",
    "  - 4: Wait\n",
    "\n",
    "Note that the target sequence has been padded with 4s to achieve the length of `maze_route_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe8c81",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e164024",
   "metadata": {},
   "source": [
    "Next, let's define the loss for the maze task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e75f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_loss(predictions, certainties, targets, cirriculum_lookahead=5, use_most_certain=True):\n",
    "    \"\"\"\n",
    "    Computes the maze loss with auto-extending cirriculum.\n",
    "\n",
    "    Predictions are of shape: (B, route_length, class, internal_ticks),\n",
    "        where classes are in [0,1,2,3,4] for [Up, Down, Left, Right, Wait]\n",
    "    Certainties are of shape: (B, 2, internal_ticks), \n",
    "        where the inside dimension (2) is [normalised_entropy, 1-normalised_entropy]\n",
    "    Targets are of shape: [B, route_length]\n",
    "\n",
    "    cirriculum_lookahead: how far to look ahead in the auto-cirriculum\n",
    "\n",
    "    use_most_certain will select either the most certain point or the final point. For baselines,\n",
    "        the final point proved the only usable option. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Predictions reshaped to: [B*route_length, 5, internal_ticks]\n",
    "    predictions_reshaped = predictions.flatten(0,1)\n",
    "    # Targets reshaped to: [B*route_length, internal_ticks]\n",
    "    targets_reshaped = torch.repeat_interleave(targets.unsqueeze(-1), \n",
    "                                               predictions.size(-1), -1).flatten(0,1).long()\n",
    "    \n",
    "    # Losses are of shape [B, route_length, internal_ticks]\n",
    "    losses = nn.CrossEntropyLoss(reduction='none')(predictions_reshaped, targets_reshaped)\n",
    "    losses = losses.reshape(predictions[:,:,0].shape)\n",
    "    \n",
    "    # Below is the code for auto-cirriculum\n",
    "    # Find where correct, and make sure to always push +5 beyond that\n",
    "    iscorrects = (predictions.argmax(2) == targets.unsqueeze(-1)).cumsum(1)\n",
    "    correct_mask = (iscorrects == torch.arange(1, iscorrects.size(1)+1, device=iscorrects.device).reshape(1, -1, 1))\n",
    "    correct_mask[:,0,:] = 1\n",
    "    upto_where = correct_mask.cumsum(1).argmax(1).max(-1)[0]+cirriculum_lookahead\n",
    "    loss_mask = torch.zeros_like(losses)\n",
    "    for bi in range(predictions.size(0)):\n",
    "        loss_mask[bi, :upto_where[bi]] = 1\n",
    "\n",
    "    # Reduce losses along route dimension\n",
    "    # Will now be of shape [B, internal_ticks]\n",
    "    losses = (losses * loss_mask).sum(1)/(loss_mask.sum(1))\n",
    "\n",
    "    loss_index_1 = losses.argmin(dim=1)\n",
    "    loss_index_2 = certainties[:,1].argmax(-1)\n",
    "    if not use_most_certain:\n",
    "        loss_index_2[:] = -1\n",
    "    \n",
    "    batch_indexer = torch.arange(predictions.size(0), device=predictions.device)\n",
    "    loss_minimum_ce = losses[batch_indexer, loss_index_1]\n",
    "    loss_selected = losses[batch_indexer, loss_index_2]\n",
    "\n",
    "    loss = ((loss_minimum_ce + loss_selected)/2).mean()\n",
    "    return loss, loss_index_2, upto_where.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb8dd7",
   "metadata": {},
   "source": [
    "We define a helper function to update the training progress bar with key metrics:\n",
    "\n",
    "Displayed Metrics:\n",
    "\n",
    "- Train Loss & Test Loss: Standard loss values during training\n",
    "- Train Acc (Step): The average accuracy for individual directional predictions across all steps in the maze trajectories\n",
    "- Statistics on the internal ticks:\n",
    "    - Average internal tick where the model has highest certainty\n",
    "    - Standard deviation of these certainty peaks\n",
    "    - Range (min ↔ max) of internal ticks where peak certainty occurs\n",
    "    - The position upto which the loss is taken, on average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb57caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pbar_desc(train_loss, train_accuracy_finegrained, test_loss, optimizer, where_most_certain, upto_where):\n",
    "    \"\"\"A helper function to create a description for the tqdm progress bar\"\"\"\n",
    "    pbar_desc = f'Train Loss={train_loss if isinstance(train_loss, float) else train_loss.item():0.3f}. Train Acc(step)={train_accuracy_finegrained:0.3f}. Test Loss={test_loss if isinstance(test_loss, float) else test_loss.item():0.3f}. LR={optimizer.param_groups[-1][\"lr\"]:0.6f}.'\n",
    "    pbar_desc += f' Where_certain={where_most_certain.float().mean().item():0.2f}+-{where_most_certain.float().std().item():0.2f} ({where_most_certain.min().item():d}<->{where_most_certain.max().item():d}). Upto: {sum(upto_where) / len(upto_where):0.2f}.'\n",
    "    return pbar_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, device='cpu', training_iterations=10000, test_every=1000, lr=1e-4, log_dir='./logs'):\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    iterator = iter(trainloader)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    with tqdm(total=training_iterations) as pbar:\n",
    "        for stepi in range(training_iterations):\n",
    "\n",
    "            try:\n",
    "                inputs, targets = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(trainloader)\n",
    "                inputs, targets = next(iterator)\n",
    "    \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions_raw, certainties, _ = model(inputs)\n",
    "\n",
    "            # Reshape: (B, SeqLength, 5, Ticks)\n",
    "            predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "            \n",
    "            # Compute loss\n",
    "            train_loss, where_most_certain, upto_where = maze_loss(predictions, certainties, targets, use_most_certain=True)\n",
    "\n",
    "            train_accuracy_finegrained = (predictions.argmax(2)[torch.arange(predictions.size(0), device=predictions.device), :, where_most_certain] == targets).float().mean().item()\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            if stepi % test_every == 0 or stepi == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_loss_per_batch = []\n",
    "                    for inputs, targets in testloader:\n",
    "                        inputs = inputs.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        \n",
    "                        predictions_raw, certainties, _ = model(inputs)\n",
    "                        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "                        \n",
    "                        test_loss, _, _ = maze_loss(predictions, certainties, targets, use_most_certain=True)\n",
    "                        test_loss_per_batch.append(test_loss.item())\n",
    "\n",
    "                    test_loss = sum(test_loss_per_batch) / len(test_loss_per_batch)\n",
    "                    test_losses.append(test_loss)\n",
    "\n",
    "                    create_maze_gif_visualization(model, testloader, device, log_dir)\n",
    "                model.train()\n",
    "\n",
    "            pbar_desc = make_pbar_desc(train_loss, train_accuracy_finegrained, test_loss, optimizer, where_most_certain, upto_where)\n",
    "            pbar.set_description(pbar_desc)\n",
    "            pbar.update(1)\n",
    "                    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def create_maze_gif_visualization(model, testloader, device, log_dir):\n",
    "    \"\"\"\n",
    "    Create GIF visualization of maze solving with attention tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a test batch\n",
    "        inputs_viz, targets_viz = next(iter(testloader))\n",
    "        inputs_viz = inputs_viz.to(device)\n",
    "        targets_viz = targets_viz.to(device)\n",
    "        \n",
    "        batch_index_to_viz = 0\n",
    "        \n",
    "        predictions_raw, certainties, _, pre_activations, post_activations, attention_tracking = model(inputs_viz, track=True)\n",
    "        \n",
    "        # Reshape predictions\n",
    "        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "        \n",
    "        # Reshape attention tracking for visualization\n",
    "        att_shape = (model.kv_features.shape[2], model.kv_features.shape[3])\n",
    "        attention_tracking = attention_tracking.reshape(attention_tracking.shape[0], attention_tracking.shape[1], -1, att_shape[0], att_shape[1])\n",
    "\n",
    "        plot_neural_dynamics(post_activations, 100, log_dir, axis_snap=True)\n",
    "        \n",
    "        # Create maze GIF with attention visualization\n",
    "        maze_input = (inputs_viz[batch_index_to_viz].detach().cpu().numpy() + 1) / 2\n",
    "        maze_predictions = predictions[batch_index_to_viz].detach().cpu().numpy()\n",
    "        maze_targets = targets_viz[batch_index_to_viz].detach().cpu().numpy()\n",
    "        maze_attention = attention_tracking[:, batch_index_to_viz] if attention_tracking.ndim > 2 else attention_tracking\n",
    "\n",
    "        # Generate the maze GIF\n",
    "        make_maze_gif(\n",
    "            maze_input,\n",
    "            maze_predictions,\n",
    "            maze_targets,\n",
    "            maze_attention,\n",
    "            log_dir\n",
    "        )\n",
    "        \n",
    "        predictions_raw, certainties, _ = model(inputs_viz)\n",
    "        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c180995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using neuron select type: random-pairing\n",
      "Synch representation size action: 32\n",
      "Synch representation size out: 32\n",
      "Model parameters: 3,304,684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:05<00:00,  8.52it/s]\n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:05<00:00,  8.61it/s]=40.88+-11.64 (18<->49). Upto: 6.81.:   1%|          | 1000/100000 [05:30<8:29:52,  3.24it/s]\n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:05<00:00,  9.68it/s]=36.78+-11.28 (16<->49). Upto: 8.25.:   2%|▏         | 2000/100000 [11:01<8:18:04,  3.28it/s]  \n",
      "Train Loss=0.928. Train Acc(step)=0.317. Test Loss=1.218. LR=0.000100. Where_certain=37.16+-10.64 (15<->49). Upto: 10.44.:   3%|▎         | 2913/100000 [16:04<8:55:50,  3.02it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     33\u001b[39m log_dir = \u001b[33m'\u001b[39m\u001b[33m./maze_training_logs\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m train_losses, test_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_dir\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, trainloader, testloader, device, training_iterations, test_every, lr, log_dir)\u001b[39m\n\u001b[32m     33\u001b[39m train_loss, where_most_certain, upto_where = maze_loss(predictions, certainties, targets, use_most_certain=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     35\u001b[39m train_accuracy_finegrained = (predictions.argmax(\u001b[32m2\u001b[39m)[torch.arange(predictions.size(\u001b[32m0\u001b[39m), device=predictions.device), :, where_most_certain] == targets).float().mean().item()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mtrain_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     40\u001b[39m train_losses.append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/atm/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/atm/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/atm/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the model\n",
    "model = CTM(\n",
    "    iterations=50,          # Number of thinking steps\n",
    "    d_model=512,            # Model dimension\n",
    "    d_input=128,             # Input dimension\n",
    "    heads=8,                # Attention heads\n",
    "    n_synch_out=32,         # Output synchronization neurons\n",
    "    n_synch_action=32,      # Action synchronization neurons\n",
    "    synapse_depth=8,        # Synapse network depth\n",
    "    memory_length=15,       # Memory length\n",
    "    deep_nlms=True,         # Use deep memory\n",
    "    memory_hidden_dims=16,  # Memory hidden dimensions\n",
    "    backbone_type='resnet34-2',  # Feature extractor\n",
    "    out_dims=50 * 5,        # Output dimensions (route_length * 5 directions)\n",
    "    prediction_reshaper=[50, 5],  # Reshape to [route_length, directions]\n",
    "    dropout=0.1,\n",
    "    do_layernorm_nlm=False,\n",
    "    positional_embedding_type='none'\n",
    ").to(device)\n",
    "\n",
    "# Initialize model parameters with dummy forward pass\n",
    "sample_batch = next(iter(trainloader))\n",
    "dummy_input = sample_batch[0][:1].to(device)\n",
    "with torch.no_grad():\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# Train the model\n",
    "log_dir = './maze_training_logs'\n",
    "train_losses, test_losses = train(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    device=device,\n",
    "    training_iterations=100000,\n",
    "    lr=1e-4,\n",
    "    log_dir=log_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
