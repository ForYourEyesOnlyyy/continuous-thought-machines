{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a72c0e",
   "metadata": {},
   "source": [
    "# The Continuous Thought Machine – Tutorial 03: Mazes [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SakanaAI/continuous-thought-machines/blob/main/examples/01_mnist.ipynb) [![arXiv](https://img.shields.io/badge/arXiv-2505.05522-b31b1b.svg)](https://arxiv.org/abs/2505.05522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cf27b",
   "metadata": {},
   "source": [
    "### Maze Solving\n",
    "\n",
    "In Section 4 of the [technical report](https://arxiv.org/pdf/2505.05522), we showcase how a CTM can be used to solve 2D mazes. Typically in the literature, the task of solving a maze is described as a form of binary classification: by ensuring the output space matches the dimensions of the input space, a model can classify, for each pixel in the input image, if the pixel belongs to the path though the maze. While this approach has seen success, it exludes the need to think in a more natural fashion. We seek to design a more challening task, where a more-human like solution is required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f769e",
   "metadata": {},
   "source": [
    "### Solving Mazes the Hard Way\n",
    "\n",
    "Instead of classifying if each pixel in the maze is or is not along the solution path, we instead constrain the output space, such that the model must output a plan: an entire trajectory of actions corresponding to the steps an agent must take to go from the start position to goal position. Specifically, for each internal tick of the CTM, the model produces a sequence of actions. These actions correspond to Left, Right, Up, Down and Wait (or no-op). We can then use a cross-entropy loss function which compares these actions produced by the CTM to the ground-truth trajectory required to solve the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbffa93",
   "metadata": {},
   "source": [
    "### Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2bbea",
   "metadata": {},
   "source": [
    "This tutorial will walk through how to train a CTM to solve mazes the hard way. It is structured as follows:\n",
    "1) Setup\n",
    "2) Data\n",
    "3) Loss Function\n",
    "4) Writing a Training Function\n",
    "5) Creating the CTM\n",
    "6) Running the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2272f18",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257dbd3",
   "metadata": {},
   "source": [
    "In addition to installing some dependencies, we also clone the CTM repo (assuming this tutorial is being ran in Colab), so that we can access the base CTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ccfdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ciaran_sakana_ai/.conda/envs/atm/lib/python3.13/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mfatal: destination path 'continuous-thought-machines' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install mediapy\n",
    "!git clone https://github.com/SakanaAI/continuous-thought-machines.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397bf56",
   "metadata": {},
   "source": [
    "Download the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=1Z8FFnZ7pZcu7DfoSyfy-ghWa08lgYl1V\"\n",
    "!unzip \"small-mazes.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab57a96",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24ffe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"./continuous-thought-machines\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# From CTM repo\n",
    "from models.ctm import ContinuousThoughtMachine as CTM\n",
    "from data.custom_datasets import MazeImageFolder\n",
    "from tasks.mazes.plotting import make_maze_gif\n",
    "from tasks.image_classification.plotting import plot_neural_dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407a4a8",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271bc4c",
   "metadata": {},
   "source": [
    "The training (test) folder contains roughly 9000 (1000) images of mazes, at a resolution of 15x15. In these images, the red and green pixels correspond to the start and goal position respectively, while the blue pixels indicate the path through the maze. We will use these blue pixels to construct targets, and remove them from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a7427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_root = './small-mazes'\n",
    "train_dir = f\"{data_root}/train/0\"\n",
    "images = os.listdir(train_dir)[:4]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, img_name in enumerate(images):\n",
    "    img = Image.open(f\"{data_root}/train/0/{img_name}\")\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec4d00",
   "metadata": {},
   "source": [
    "To use these mazes, we have built a custom dataset `MazeImageFolder`. This dataset creates the input-target pairs corresponding to the unsolved mazes images and their solutions. For more details on how this dataset process the images, please refer to it's implementation [here](https://github.com/SakanaAI/continuous-thought-machines/blob/aecfb63ac42db7a20903ee27489ff71671669474/data/custom_datasets.py#L142C7-L142C22).\n",
    "\n",
    "Note that the `MazeImageFolder` takes a `maze_route_length` argument. This parameter specifies the fixed length of action sequences that the CTM will generate at each internal tick. Since different mazes have solutions of varying lengths, all target sequences are padded with the wait action (integer 4) to ensure they match the specified `maze_route_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc28def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes:   1%|          | 85/9000 [00:00<00:18, 476.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes: 100%|██████████| 9000/9000 [00:20<00:00, 439.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving all mazes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mazes: 100%|██████████| 1000/1000 [00:02<00:00, 494.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving all mazes...\n"
     ]
    }
   ],
   "source": [
    "train_data = MazeImageFolder(root=f'{data_root}/train/', which_set='train', maze_route_length=50)\n",
    "test_data = MazeImageFolder(root=f'{data_root}/test/', which_set='test', maze_route_length=50)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a149b",
   "metadata": {},
   "source": [
    "We can visualise what these inputs and targets look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33503097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target action sequence:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "sample_inputs, sample_targets = next(iter(testloader))\n",
    "sample_input, sample_target = sample_inputs[0], sample_targets[0]\n",
    "sample_input = sample_input.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(np.clip(sample_input, 0, 1))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Target action sequence:\")\n",
    "print(f\"{sample_target.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d617604",
   "metadata": {},
   "source": [
    "As shown in the above output, the target is a sequence of integers, each corresponding to an action: \n",
    "  - 0: Up\n",
    "  - 1: Down  \n",
    "  - 2: Left\n",
    "  - 3: Right\n",
    "  - 4: Wait\n",
    "\n",
    "Note that the target sequence has been padded with 4s to achieve the desired length of `maze_route_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe8c81",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e164024",
   "metadata": {},
   "source": [
    "Next, we define the loss function. We use the cross-entropy loss but there are two main points to highlight:\n",
    "\n",
    "1) We only use the loss at **two specific internal ticks**. These correspond to the internal tick which achieved the lowest cross-entropy loss of all internal ticks (`loss_index_1` below), and the internal tick at which the model has the highest certainty (`loss_index_2` below). Becaues these indices will vary across batch elements we define a `batch_indexer` which we can use for advanced indexing into the `losses` tensor which stores the loss at all internal ticks.\n",
    "\n",
    "2) Our loss calculation uses an auto-extending cirriculum. Specifically, for each trajectory (recall the CTM produces an entire trajectory for every internal tick), we find the index of the first action that the CTM got wrong, and then **only include in our loss the trajectory up to and extending `cirriculum_lookahead` actions beyond this first incorrect action**. In this way, the CTM initially focuses on correctly predicting the first actions, and as it learns, it gets better at producing later and later actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63e75f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_loss(predictions, certainties, targets, cirriculum_lookahead=5, use_most_certain=True):\n",
    "    \"\"\"\n",
    "    Computes the maze loss with auto-extending cirriculum.\n",
    "\n",
    "    Predictions are of shape: (B, route_length, class, internal_ticks),\n",
    "        where classes are in [0,1,2,3,4] for [Up, Down, Left, Right, Wait]\n",
    "    Certainties are of shape: (B, 2, internal_ticks), \n",
    "        where the inside dimension (2) is [normalised_entropy, 1-normalised_entropy]\n",
    "    Targets are of shape: [B, route_length]\n",
    "\n",
    "    cirriculum_lookahead: how far to look ahead in the auto-cirriculum\n",
    "\n",
    "    use_most_certain will select either the most certain point or the final point. For baselines,\n",
    "        the final point proved the only usable option. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Predictions reshaped to: [B*route_length, 5, internal_ticks]\n",
    "    predictions_reshaped = predictions.flatten(0,1)\n",
    "    # Targets reshaped to: [B*route_length, internal_ticks]\n",
    "    targets_reshaped = torch.repeat_interleave(targets.unsqueeze(-1), \n",
    "                                               predictions.size(-1), -1).flatten(0,1).long()\n",
    "    \n",
    "    # Losses are of shape [B, route_length, internal_ticks]\n",
    "    losses = nn.CrossEntropyLoss(reduction='none')(predictions_reshaped, targets_reshaped)\n",
    "    losses = losses.reshape(predictions[:,:,0].shape)\n",
    "    \n",
    "    # Below is the code for auto-cirriculum\n",
    "    # Find where correct, and make sure to always push +5 beyond that\n",
    "    iscorrects = (predictions.argmax(2) == targets.unsqueeze(-1)).cumsum(1)\n",
    "    correct_mask = (iscorrects == torch.arange(1, iscorrects.size(1)+1, device=iscorrects.device).reshape(1, -1, 1))\n",
    "    correct_mask[:,0,:] = 1\n",
    "    upto_where = correct_mask.cumsum(1).argmax(1).max(-1)[0]+cirriculum_lookahead\n",
    "    loss_mask = torch.zeros_like(losses)\n",
    "    for bi in range(predictions.size(0)):\n",
    "        loss_mask[bi, :upto_where[bi]] = 1\n",
    "\n",
    "    # Reduce losses along route dimension\n",
    "    # Will now be of shape [B, internal_ticks]\n",
    "    losses = (losses * loss_mask).sum(1)/(loss_mask.sum(1))\n",
    "\n",
    "    loss_index_1 = losses.argmin(dim=1)\n",
    "    loss_index_2 = certainties[:,1].argmax(-1)\n",
    "    if not use_most_certain:\n",
    "        loss_index_2[:] = -1\n",
    "    \n",
    "    batch_indexer = torch.arange(predictions.size(0), device=predictions.device)\n",
    "    loss_minimum_ce = losses[batch_indexer, loss_index_1]\n",
    "    loss_selected = losses[batch_indexer, loss_index_2]\n",
    "\n",
    "    loss = ((loss_minimum_ce + loss_selected)/2).mean()\n",
    "    return loss, loss_index_2, upto_where.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712a9a9",
   "metadata": {},
   "source": [
    "### Writing a Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb8dd7",
   "metadata": {},
   "source": [
    "We define a helper function to update the training progress bar with key metrics:\n",
    "\n",
    "Displayed Metrics:\n",
    "\n",
    "- Train Loss & Test Loss: Standard loss values during training\n",
    "- Train Acc (Step): The average accuracy for individual directional predictions across all steps in the maze trajectories\n",
    "- Statistics on the internal ticks:\n",
    "    - Average internal tick where the model has highest certainty\n",
    "    - Standard deviation of these certainty peaks\n",
    "    - Range (min ↔ max) of internal ticks where peak certainty occurs\n",
    "    - The position upto which the loss is taken, on average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb57caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pbar_desc(train_loss, train_accuracy_finegrained, test_loss, optimizer, where_most_certain, upto_where):\n",
    "    \"\"\"A helper function to create a description for the tqdm progress bar\"\"\"\n",
    "    pbar_desc = f'Train Loss={train_loss if isinstance(train_loss, float) else train_loss.item():0.3f}. Train Acc(step)={train_accuracy_finegrained:0.3f}. Test Loss={test_loss if isinstance(test_loss, float) else test_loss.item():0.3f}. LR={optimizer.param_groups[-1][\"lr\"]:0.6f}.'\n",
    "    pbar_desc += f' Where_certain={where_most_certain.float().mean().item():0.2f}+-{where_most_certain.float().std().item():0.2f} ({where_most_certain.min().item():d}<->{where_most_certain.max().item():d}). Upto: {sum(upto_where) / len(upto_where):0.2f}.'\n",
    "    return pbar_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, device='cpu', training_iterations=10000, test_every=1000, lr=1e-4, log_dir='./logs'):\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    iterator = iter(trainloader)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    with tqdm(total=training_iterations) as pbar:\n",
    "        for stepi in range(training_iterations):\n",
    "\n",
    "            try:\n",
    "                inputs, targets = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(trainloader)\n",
    "                inputs, targets = next(iterator)\n",
    "    \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions_raw, certainties, _ = model(inputs)\n",
    "\n",
    "            # Reshape: (B, SeqLength, 5, Ticks)\n",
    "            predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "            \n",
    "            # Compute loss\n",
    "            train_loss, where_most_certain, upto_where = maze_loss(predictions, certainties, targets, use_most_certain=True)\n",
    "\n",
    "            train_accuracy_finegrained = (predictions.argmax(2)[torch.arange(predictions.size(0), device=predictions.device), :, where_most_certain] == targets).float().mean().item()\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            if stepi % test_every == 0 or stepi == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_loss_per_batch = []\n",
    "                    for inputs, targets in testloader:\n",
    "                        inputs = inputs.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        \n",
    "                        predictions_raw, certainties, _ = model(inputs)\n",
    "                        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "                        \n",
    "                        test_loss, _, _ = maze_loss(predictions, certainties, targets, use_most_certain=True)\n",
    "                        test_loss_per_batch.append(test_loss.item())\n",
    "\n",
    "                    test_loss = sum(test_loss_per_batch) / len(test_loss_per_batch)\n",
    "                    test_losses.append(test_loss)\n",
    "\n",
    "                    create_maze_gif_visualization(model, testloader, device, log_dir)\n",
    "                model.train()\n",
    "\n",
    "            pbar_desc = make_pbar_desc(train_loss, train_accuracy_finegrained, test_loss, optimizer, where_most_certain, upto_where)\n",
    "            pbar.set_description(pbar_desc)\n",
    "            pbar.update(1)\n",
    "                    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def create_maze_gif_visualization(model, testloader, device, log_dir):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs_viz, targets_viz = next(iter(testloader))\n",
    "        inputs_viz = inputs_viz.to(device)\n",
    "        targets_viz = targets_viz.to(device)\n",
    "        \n",
    "        batch_index_to_viz = 0\n",
    "        \n",
    "        predictions_raw, certainties, _, pre_activations, post_activations, attention_tracking = model(inputs_viz, track=True)\n",
    "        \n",
    "        # Reshape predictions\n",
    "        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n",
    "        \n",
    "        # Reshape attention tracking for visualization\n",
    "        att_shape = (model.kv_features.shape[2], model.kv_features.shape[3])\n",
    "        attention_tracking = attention_tracking.reshape(attention_tracking.shape[0], attention_tracking.shape[1], -1, att_shape[0], att_shape[1])\n",
    "\n",
    "        plot_neural_dynamics(post_activations, 100, log_dir, axis_snap=True)\n",
    "        \n",
    "        # Create maze GIF with attention visualization\n",
    "        maze_input = (inputs_viz[batch_index_to_viz].detach().cpu().numpy() + 1) / 2\n",
    "        maze_predictions = predictions[batch_index_to_viz].detach().cpu().numpy()\n",
    "        maze_targets = targets_viz[batch_index_to_viz].detach().cpu().numpy()\n",
    "        maze_attention = attention_tracking[:, batch_index_to_viz] if attention_tracking.ndim > 2 else attention_tracking\n",
    "\n",
    "        # Generate the maze GIF\n",
    "        make_maze_gif(\n",
    "            maze_input,\n",
    "            maze_predictions,\n",
    "            maze_targets,\n",
    "            maze_attention,\n",
    "            log_dir\n",
    "        )\n",
    "        \n",
    "        predictions_raw, certainties, _ = model(inputs_viz)\n",
    "        predictions = predictions_raw.reshape(predictions_raw.size(0), -1, 5, predictions_raw.size(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6c8fc",
   "metadata": {},
   "source": [
    "### Creating the CTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c180995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using neuron select type: random-pairing\n",
      "Synch representation size action: 32\n",
      "Synch representation size out: 32\n",
      "Model parameters: 3,304,684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:04<00:00, 10.38it/s]\n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:05<00:00,  9.85it/s]=42.53+-9.82 (24<->49). Upto: 7.91.:   1%|          | 1000/100000 [05:30<8:29:58,  3.24it/s]\n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:05<00:00,  9.27it/s]=42.91+-8.97 (25<->49). Upto: 11.19.:   2%|▏         | 2000/100000 [11:02<8:20:58,  3.26it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:06<00:00,  8.21it/s]=42.59+-7.85 (22<->49). Upto: 16.88.:   3%|▎         | 3000/100000 [16:30<8:19:43,  3.24it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:07<00:00,  7.06it/s]=39.34+-8.30 (21<->49). Upto: 16.12.:   4%|▍         | 4000/100000 [22:05<8:15:34,  3.23it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 15.02it/s]=36.91+-6.23 (25<->49). Upto: 17.16.:   5%|▌         | 5000/100000 [27:38<8:06:23,  3.26it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.89it/s]=33.69+-6.98 (19<->49). Upto: 18.97.:   6%|▌         | 6000/100000 [33:11<8:06:05,  3.22it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 15.00it/s]=31.72+-10.09 (15<->49). Upto: 15.16.:   7%|▋         | 7000/100000 [38:39<7:51:28,  3.29it/s] \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:09<00:00,  5.29it/s]=33.00+-11.57 (18<->49). Upto: 15.66.:   8%|▊         | 8000/100000 [44:07<7:46:30,  3.29it/s] \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.78it/s]=30.62+-8.35 (22<->49). Upto: 21.00.:   9%|▉         | 9000/100000 [49:36<7:43:53,  3.27it/s]   \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.77it/s]=34.34+-10.29 (18<->49). Upto: 20.06.:  10%|█         | 10000/100000 [55:13<7:46:44,  3.21it/s] \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.61it/s]=30.88+-10.18 (14<->49). Upto: 20.72.:  11%|█         | 11000/100000 [1:00:43<7:33:33,  3.27it/s]\n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.89it/s]=28.66+-9.99 (14<->49). Upto: 21.50.:  12%|█▏        | 12000/100000 [1:06:21<7:30:03,  3.26it/s]   \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.60it/s]=32.72+-12.38 (15<->49). Upto: 17.50.:  13%|█▎        | 13000/100000 [1:11:50<7:27:18,  3.24it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.56it/s]=30.25+-10.59 (16<->49). Upto: 20.50.:  14%|█▍        | 14000/100000 [1:17:19<7:21:52,  3.24it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.26it/s]=27.00+-11.36 (12<->48). Upto: 26.59.:  15%|█▌        | 15000/100000 [1:23:02<7:22:33,  3.20it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.04it/s]=33.66+-10.12 (14<->47). Upto: 24.66.:  16%|█▌        | 16000/100000 [1:28:36<7:17:52,  3.20it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 14.06it/s]=31.97+-11.64 (12<->49). Upto: 23.78.:  17%|█▋        | 17000/100000 [1:34:13<7:13:20,  3.19it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 13.98it/s]=28.47+-10.35 (13<->46). Upto: 21.44.:  18%|█▊        | 18000/100000 [1:40:00<7:09:42,  3.18it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 13.92it/s]=27.28+-9.17 (13<->47). Upto: 32.03.:  19%|█▉        | 19000/100000 [1:45:38<7:02:14,  3.20it/s]   \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:17<00:00,  2.79it/s]=29.22+-10.84 (14<->49). Upto: 33.62.:  20%|██        | 20000/100000 [1:51:18<6:57:58,  3.19it/s]  \n",
      "Processing frames for maze plotting: 100%|██████████| 50/50 [00:03<00:00, 13.79it/s]=33.25+-10.51 (12<->48). Upto: 33.12.:  21%|██        | 21000/100000 [1:57:12<6:51:30,  3.20it/s] \n",
      "Train Loss=0.321. Train Acc(step)=0.736. Test Loss=0.471. LR=0.000100. Where_certain=32.28+-10.97 (15<->48). Upto: 29.75.:  22%|██▏       | 21954/100000 [2:02:36<6:50:55,  3.17it/s]  "
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the model\n",
    "model = CTM(\n",
    "    iterations=50,          # Number of thinking steps\n",
    "    d_model=512,            # Model dimension\n",
    "    d_input=128,             # Input dimension\n",
    "    heads=8,                # Attention heads\n",
    "    n_synch_out=32,         # Output synchronization neurons\n",
    "    n_synch_action=32,      # Action synchronization neurons\n",
    "    synapse_depth=8,        # Synapse network depth\n",
    "    memory_length=15,       # Memory length\n",
    "    deep_nlms=True,         # Use deep memory\n",
    "    memory_hidden_dims=16,  # Memory hidden dimensions\n",
    "    backbone_type='resnet34-2',  # Feature extractor\n",
    "    out_dims=50 * 5,        # Output dimensions (route_length * 5 directions)\n",
    "    prediction_reshaper=[50, 5],  # Reshape to [route_length, directions]\n",
    "    dropout=0.1,\n",
    "    do_layernorm_nlm=False,\n",
    "    positional_embedding_type='none'\n",
    ").to(device)\n",
    "\n",
    "# Initialize model parameters with dummy forward pass\n",
    "sample_batch = next(iter(trainloader))\n",
    "dummy_input = sample_batch[0][:1].to(device)\n",
    "with torch.no_grad():\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# Train the model\n",
    "log_dir = './maze_training_logs'\n",
    "train_losses, test_losses = train(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    device=device,\n",
    "    training_iterations=100000,\n",
    "    lr=1e-4,\n",
    "    log_dir=log_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
